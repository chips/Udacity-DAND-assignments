{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Rate Dogs. Wrangling and analysis project\n",
    "\n",
    "First, we're getting the basics out of the way...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import datetime\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather files and resources as needed for the project.\n",
    "This means loading the files supplied and downloading the image predictions.\n",
    "The resources we need to get are:\n",
    "- The archive 'twitter-archive-enhanced.csv' from udacity. Downloaded and added to the project\n",
    "- The image predictions files which will be programatically downloaded\n",
    "- info for all the tweets, accessed from through the twitter api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>retweeted_status_user_id</th>\n",
       "      <th>retweeted_status_timestamp</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>rating_numerator</th>\n",
       "      <th>rating_denominator</th>\n",
       "      <th>name</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-01 16:23:56 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Phineas. He's a mystical boy. Only eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/892420643...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Phineas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>892177421306343426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-01 00:17:27 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Tilly. She's just checking pup on you....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/892177421...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Tilly</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>891815181378084864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-31 00:18:03 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Archie. He is a rare Norwegian Pouncin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/891815181...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Archie</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>891689557279858688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-30 15:58:51 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Darla. She commenced a snooze mid meal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/891689557...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Darla</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>891327558926688256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-29 16:00:24 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Franklin. He would like you to stop ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/891327558...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  in_reply_to_status_id  in_reply_to_user_id  \\\n",
       "0  892420643555336193                    NaN                  NaN   \n",
       "1  892177421306343426                    NaN                  NaN   \n",
       "2  891815181378084864                    NaN                  NaN   \n",
       "3  891689557279858688                    NaN                  NaN   \n",
       "4  891327558926688256                    NaN                  NaN   \n",
       "\n",
       "                   timestamp  \\\n",
       "0  2017-08-01 16:23:56 +0000   \n",
       "1  2017-08-01 00:17:27 +0000   \n",
       "2  2017-07-31 00:18:03 +0000   \n",
       "3  2017-07-30 15:58:51 +0000   \n",
       "4  2017-07-29 16:00:24 +0000   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                                text  retweeted_status_id  \\\n",
       "0  This is Phineas. He's a mystical boy. Only eve...                  NaN   \n",
       "1  This is Tilly. She's just checking pup on you....                  NaN   \n",
       "2  This is Archie. He is a rare Norwegian Pouncin...                  NaN   \n",
       "3  This is Darla. She commenced a snooze mid meal...                  NaN   \n",
       "4  This is Franklin. He would like you to stop ca...                  NaN   \n",
       "\n",
       "   retweeted_status_user_id retweeted_status_timestamp  \\\n",
       "0                       NaN                        NaN   \n",
       "1                       NaN                        NaN   \n",
       "2                       NaN                        NaN   \n",
       "3                       NaN                        NaN   \n",
       "4                       NaN                        NaN   \n",
       "\n",
       "                                       expanded_urls  rating_numerator  \\\n",
       "0  https://twitter.com/dog_rates/status/892420643...                13   \n",
       "1  https://twitter.com/dog_rates/status/892177421...                13   \n",
       "2  https://twitter.com/dog_rates/status/891815181...                12   \n",
       "3  https://twitter.com/dog_rates/status/891689557...                13   \n",
       "4  https://twitter.com/dog_rates/status/891327558...                12   \n",
       "\n",
       "   rating_denominator      name doggo floofer pupper puppo  \n",
       "0                  10   Phineas  None    None   None  None  \n",
       "1                  10     Tilly  None    None   None  None  \n",
       "2                  10    Archie  None    None   None  None  \n",
       "3                  10     Darla  None    None   None  None  \n",
       "4                  10  Franklin  None    None   None  None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Files\n",
    "\n",
    "#Read the csv file and have a look at it.\n",
    "\n",
    "archive = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666020888022790149</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Welsh_springer_spaniel</td>\n",
       "      <td>0.465074</td>\n",
       "      <td>True</td>\n",
       "      <td>collie</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>True</td>\n",
       "      <td>Shetland_sheepdog</td>\n",
       "      <td>0.061428</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>666029285002620928</td>\n",
       "      <td>https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.074192</td>\n",
       "      <td>True</td>\n",
       "      <td>Rhodesian_ridgeback</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>666033412701032449</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>German_shepherd</td>\n",
       "      <td>0.596461</td>\n",
       "      <td>True</td>\n",
       "      <td>malinois</td>\n",
       "      <td>0.138584</td>\n",
       "      <td>True</td>\n",
       "      <td>bloodhound</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>666044226329800704</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5Dr8HUEAA-lEu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rhodesian_ridgeback</td>\n",
       "      <td>0.408143</td>\n",
       "      <td>True</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.360687</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666049248165822465</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.560311</td>\n",
       "      <td>True</td>\n",
       "      <td>Rottweiler</td>\n",
       "      <td>0.243682</td>\n",
       "      <td>True</td>\n",
       "      <td>Doberman</td>\n",
       "      <td>0.154629</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                          jpg_url  \\\n",
       "0  666020888022790149  https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg   \n",
       "1  666029285002620928  https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg   \n",
       "2  666033412701032449  https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg   \n",
       "3  666044226329800704  https://pbs.twimg.com/media/CT5Dr8HUEAA-lEu.jpg   \n",
       "4  666049248165822465  https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg   \n",
       "\n",
       "   img_num                      p1   p1_conf  p1_dog                  p2  \\\n",
       "0        1  Welsh_springer_spaniel  0.465074    True              collie   \n",
       "1        1                 redbone  0.506826    True  miniature_pinscher   \n",
       "2        1         German_shepherd  0.596461    True            malinois   \n",
       "3        1     Rhodesian_ridgeback  0.408143    True             redbone   \n",
       "4        1      miniature_pinscher  0.560311    True          Rottweiler   \n",
       "\n",
       "    p2_conf  p2_dog                   p3   p3_conf  p3_dog  \n",
       "0  0.156665    True    Shetland_sheepdog  0.061428    True  \n",
       "1  0.074192    True  Rhodesian_ridgeback  0.072010    True  \n",
       "2  0.138584    True           bloodhound  0.116197    True  \n",
       "3  0.360687    True   miniature_pinscher  0.222752    True  \n",
       "4  0.243682    True             Doberman  0.154629    True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download the tsv file for image predictions\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(url.split('/')[-1], mode='wb') as file:\n",
    "    file.write(response.content)\n",
    "    \n",
    "img_pred = pd.read_csv('image-predictions.tsv', sep='\\t')\n",
    "img_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter Stuff - Api access\n",
    "\n",
    "import tweepy\n",
    "\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "access_token = 'HIDDEN'\n",
    "access_secret = 'HIDDEN'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TweepError",
     "evalue": "[{'code': 89, 'message': 'Invalid or expired token.'}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b9ad53fcde4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Testing that Twitter works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'extended'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTweepError\u001b[0m: [{'code': 89, 'message': 'Invalid or expired token.'}]"
     ]
    }
   ],
   "source": [
    "#Testing that Twitter works\n",
    "tweet = api.get_status(archive.tweet_id[1], tweet_mode='extended')\n",
    "info = tweet._json\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the tweets\n",
    "#Saving out the errors in a txt file, just in case\n",
    "\n",
    "#set this in order to grab or not grab the data from twitter\n",
    "getData = 0\n",
    "if(getData==1):\n",
    "    data = []\n",
    "    problem_tweets = {}\n",
    "    tweetcount =archive.tweet_id.size\n",
    "    #tweetcount = 40 #This one is just for troubleshooting\n",
    "\n",
    "    for a in range(0, tweetcount):\n",
    "        currentid = archive.tweet_id[a]\n",
    "        print(\"requesting tweet id: {} at {}\".format(currentid, datetime.datetime.now().time()))\n",
    "        try:\n",
    "            tweet = api.get_status(archive.tweet_id[a], tweet_mode='extended')\n",
    "            data.append(tweet._json)\n",
    "        except Exception as e:\n",
    "            print(\"failed tweet id: {} at {}\".format(currentid, datetime.datetime.now().time()))    \n",
    "            problem_tweets[currentid] = str(e)\n",
    "\n",
    "    with open(\"tweet_json.txt\", mode='w') as file:\n",
    "        json.dump(data, file)\n",
    "    \n",
    "    #Saving the errors, just in case we need them at some point...\n",
    "    f = open(\"tweet_errors.txt\",\"w\")\n",
    "    f.write(str(problem_tweets))\n",
    "    f.close()\n",
    "\n",
    "    print(\"done\")\n",
    "    print(\"issues with tweets: {}\".format(problem_tweets))\n",
    "elif(getData==0):\n",
    "    print(\"Not grabbing data, assuming you already have it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn the twitter data into something we can use\n",
    "I want to store these fields:\n",
    "- tweet id (required)\n",
    "- retweet count (required)\n",
    "- favorite count (required)\n",
    "- followers count (already formulating ideas that it could be interesting to see if followe counts have a relative impact on favorites and retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn json into dataframe.\n",
    "\n",
    "#Load and parse the file\n",
    "tweet_list = []\n",
    "with open('tweet_json.txt') as file:\n",
    "    alltweets = json.load(file)\n",
    "    for tweet in alltweets:\n",
    "        tweet_list.append({'tweet_id' : tweet['id'], 'retweet_count' : tweet['retweet_count'],\n",
    "                          'favorite_count': tweet['favorite_count'], 'followers_count' : tweet['user']['followers_count']})\n",
    "\n",
    "#Turn the file into a dataframe\n",
    "tw_api_data = pd.DataFrame(tweet_list, columns = ['tweet_id', 'retweet_count', 'favorite_count',\n",
    "                                                 'followers_count'])\n",
    "\n",
    "#Quickly have a look to see that all is going according to plan\n",
    "tw_api_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering roundup\n",
    "\n",
    "We now have three data sources in dataframes. \n",
    "- _tw_api_data_ holds the data we extrated from the twitter API.\n",
    "- _archive_ The data uploaded, from the file supplied by udacity. \n",
    "- _img_pred_ The predictions from the tab seperated file downloaded programatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the data prior to cleaning\n",
    "Now that all the sources are in place, I want to look at them and see what needs cleaning, tidying and wrangling.\n",
    "\n",
    "I will start with the Archive... There are things I know we need to pay attention to:\n",
    "- Dog name\n",
    "- Stages\n",
    "- Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the top. The Archive\n",
    "\n",
    "print(archive.info())\n",
    "archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick check to see how a row looks\n",
    "\n",
    "print(archive.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we don't need the source column, and we can see that the link is embedded in the text field as well. This quick look also made me interested to see if expanded_urls sometimes hold more than one url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.expanded_urls.str.split(',')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "it seems to be the case... maybe not something to worry about yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many names have been captured\n",
    "\n",
    "archive['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This introduces quite a few things we need to look at already, lets start the list:\n",
    "745 names are missing\n",
    "8 dogs called the, and 55 dogs called a\n",
    "\n",
    "So we need to find those names, and I suspect that filtering for lowercase names will give me a list of dogs that need special attention as well. I will have a quick look at this straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerFrame = archive[archive['name'].str[0].str.islower()]\n",
    "lowerFrame['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so I will make the assumption that nobody calls their dog 'quite' or 'this' ... though 'Unaccaptable' and 'Mad' could be funny names. But in all seriousness, we need to address these.\n",
    "\n",
    "# Dog stages\n",
    "Next up we look into the stages of the dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(archive.loc[:,'doggo':'puppo'] != 'None').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of curiosity I want to see if there are dogs that have been classified with more than one type\n",
    "multidogs = 0\n",
    "unclasdogs = 0\n",
    "singleclasdogs = 0\n",
    "for index, row in archive.iterrows():\n",
    "    dogtypecount = 0\n",
    "    if row['doggo'] != 'None':\n",
    "        dogtypecount += 1\n",
    "    if row['floofer'] != 'None':\n",
    "        dogtypecount += 1\n",
    "    if row['pupper'] != 'None':\n",
    "        dogtypecount += 1\n",
    "    if row['puppo'] != 'None':\n",
    "        dogtypecount += 1\n",
    "    if dogtypecount >=2:\n",
    "        multidogs += 1\n",
    "    elif dogtypecount ==1:\n",
    "        singleclasdogs += 1\n",
    "    else:\n",
    "        unclasdogs += 1\n",
    "\n",
    "print(\"there are {} dogs with more than one classification\".format( multidogs))\n",
    "print(\"there are {} dogs with just  one classification\".format( singleclasdogs))\n",
    "print(\"there are {} dogs with no classification\".format( unclasdogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so there are already a lot of things to look at, but let us continue the assesment.\n",
    "\n",
    "# Dog ratings\n",
    "\n",
    "time to asses the ratings. These are the most important features, if the name of the account is to be taken seriously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive['rating_numerator'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive['rating_denominator'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "allright.... it's all over the place. There is no missing data it seems, but people seem liberal in keeping to actual ratings that would make sense. But we could likely find interesting things from this at some point. \n",
    "\n",
    "The last thing I want to look at is to check if we have any retweets or replies in there that we should get rid of. \n",
    "\n",
    "# No retweets or replies!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to see a bit more\n",
    "pd.set_option('max_colwidth',280)\n",
    "\n",
    "#lets look at the text in some of the replies\n",
    "archive[archive.in_reply_to_user_id.notnull()].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and the same for the tweets that have been retweeted\n",
    "archive[archive.retweeted_status_user_id .notnull()].text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From reading through quite a few of both outputs, I will deem these rows unusable.\n",
    "\n",
    "# Quality and tidiness issues to fix on the Archive dataframe:\n",
    "\n",
    "Quality:\n",
    "- remove rows that are replies or retweets.\n",
    "- Work on classification of dogs\n",
    "- Remove names with lowercase start\n",
    "- Find dog names where missing and possible\n",
    "- Clean links from text\n",
    "\n",
    "\n",
    "Tidiness:\n",
    "- I want to clean the categorization. Having the row name and variable be the same is redundant and annoying\n",
    "- Once we have removed the retweets and replies, we can get rid of those columns.\n",
    "\n",
    "Now let's move on to the images and see what other things we can find...\n",
    "\n",
    "# img_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting an overview...\n",
    "img_pred.sample(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, there are some things we need to look into here. \n",
    "\n",
    "Ofcourse we need to merge this into the archive data, but we need to clean some things first.\n",
    "- it doesnt seem like we need the img_num column\n",
    "- There are quite a few predictions that are not dogs. we need to see how many have non-dog predictions in all three predictions. \n",
    "- We should only have on prediction per entry so probably the highest rated one would make sense.\n",
    "- A quick sanity check that we do not have missing fields etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first things first...\n",
    "img_pred.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that springs to mind is that we only have 2075 entries. This needs to be dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see how many entries to not have a dog in any prediction.\n",
    "notDog = 0\n",
    "for index, row in img_pred.iterrows():\n",
    "    if ((int(row['p1_dog'])+int(row['p2_dog'])+int(row['p3_dog'])) == 0):\n",
    "        notDog += 1\n",
    "\n",
    "print(notDog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that seems pretty straight forward. We do need to choose a way to move forward with regards to the dogs images that are not classified as dogs. Looking through some of them, it seems the prediction is actually right in many cases, so it might make sense to get rid of those. Another option is to just not have anything in that field.\n",
    "\n",
    "# Quality and tidiness issues to fix on the img_pred dataframe:\n",
    "\n",
    "Quality:\n",
    "- keep just the highest confidence prediction that is also a dog. (let's keep the confidence as well)\n",
    "\n",
    "Tidiness:\n",
    "- get rid of the img_num column\n",
    "- fix upper and lower cases\n",
    "\n",
    "Other things:\n",
    "- once merged, we need to see what the issue is with missing predictions.\n",
    "\n",
    "# Twitter api download\n",
    "\n",
    "Time to move on to asses the newly downloaded data. Again, this one is mostly sanity checking, so let's get to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_api_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets have a look at some histograms to see if we can learn more...\n",
    "\n",
    "tw_api_data.hist(bins=40,figsize=(16,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing too surprising here. The most interesting thing as I see it, but this gets into analysis, is that it seems there is a very 'all or nothing' distribution to observe, though this is a pretty normal long tail. \n",
    "I am actually happy with this dataframe, I just need to merge it into the other data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data! \n",
    "\n",
    "It's now time to start cleaning. The issues I want to deal with are:\n",
    "\n",
    "_Archive_\n",
    "- remove rows that are retweets.\n",
    "- remove rows that are replies.\n",
    "- remove columns related to replies and retweets\n",
    "- Dog names. I need to clean the ones that have been wrongly extracted. And try to extract more.\n",
    "- Classification. Just one per dog. And try to extract more where possible\n",
    "- Classifaction will go in one column\n",
    "- remove urls from text.\n",
    "\n",
    "_image predictions_\n",
    "- remove predictions for non-dogs\n",
    "- have just one prediction with a confidence level per dog\n",
    "- remove img_num column\n",
    "- fix upper and lower cases in prediction names\n",
    "\n",
    "\n",
    "Next up we need to merge everything into one nice, big dataframe and have a look at that. I know there will be an issue of some entries missing that we need to look into. So let's get started.\n",
    "\n",
    "# Wrangling the Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__: \n",
    "\n",
    "1- Drop duplicates, if there are any, as a first step. This way we avoid working on more data than we need. Then create a new dataframe that we can keep working with.\n",
    "\n",
    "__Code__;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the simplest part of cleaning in order to not spend time on things that will be removed.\n",
    "\n",
    "print(archive.duplicated().sum())\n",
    "archive_new = archive.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_new.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__: \n",
    "\n",
    "2- Remove rows that are retweets. These have values in the retweeted_status_user_id column, so using that as an identifier\n",
    "\n",
    "__Code__;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that are retweets\n",
    "archive_new = archive_new[archive_new.retweeted_status_user_id.isnull()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check it\n",
    "archive_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__: \n",
    "\n",
    "3- Remove rows that are replies. These have values in the reply_to_user_id column, so using that as an identifier\n",
    "\n",
    "__Code__;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that are replies\n",
    "archive_new = archive_new[archive_new.in_reply_to_user_id.isnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check it\n",
    "archive_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__: \n",
    "\n",
    "4- Get rid of the now empty columns for replies and rewteets.\n",
    "\n",
    "__Code__;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of the empty columns and source column while we are at it.\n",
    "\n",
    "archive_new.drop(['in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id',\n",
    "                 'retweeted_status_user_id','retweeted_status_timestamp','source'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__: \n",
    "\n",
    "5- Remove the names if they start with a lower case letter\n",
    "\n",
    "__Code__;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to remove all the lowercase names, as we found earlier that these were not names.\n",
    "\n",
    "for index, row in archive_new.iterrows():\n",
    "    if row['name'][0].islower():\n",
    "        print(index)\n",
    "        archive_new.drop(index, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerStart = 0\n",
    "for index, row in archive_new.iterrows():\n",
    "    if row['name'][0].islower():\n",
    "        lowerStart += 1\n",
    "\n",
    "print(\"There are {} names that start with a lowercase letter\".format(lowerStart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__: \n",
    "\n",
    "6- Change the names that are ste to None to actual blanks. This will make it easer to search and gather an overview etc.\n",
    "\n",
    "__Code__;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the names that are None to actual blanks\n",
    "for index, row in archive_new.iterrows():\n",
    "    if row['name'] == \"None\":\n",
    "        archive_new.loc[index, 'name'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_new.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',180) #Making things easier to see\n",
    "\n",
    "#And just to make sure that worked and have a look at the text fields\n",
    "\n",
    "archive_new[archive_new.name.isnull()].text.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately it doesnt seem like these have names that can be extracted automatically, so I guess we need to have a lot of dogs without names. At least we've taken out many of the errornurous names.\n",
    "\n",
    "__Define__:\n",
    "\n",
    "7- Remove links from the text fields to simplify and avoid whatever things could cause errors later on when extracting data from the field.\n",
    "\n",
    "__Code__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing links from text fields\n",
    "for index, row in archive_new.iterrows():\n",
    "    archive_new.loc[index, 'text'] = row['text'].rsplit('http', 1)[0].rsplit('http', 1)[0]\n",
    "\n",
    "archive_new.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, that takes care of that. Now I 'just' need to wrangle the stages. I just want to make sure we have one column, and that has the name of the stage in it, and is empty if not identified. Since a floofer can also be a catch-all for just about any dog, that has lowest priority in case there are more than one stage noted for a tweet. Next comes Puppo, then pupper and lastly doggo. Mainly because the description describes them as inhereting traits for from the former stage.\n",
    "\n",
    "__Define__:\n",
    "\n",
    "8- Clean up dog stages. The current configuration is not making life easer for us, so the stages will go into one single column, instead of four, and in the process we are getting rid of multiple classifications per dog.\n",
    "\n",
    "__Code__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_new['dog_stage'] = None\n",
    "for index, row in archive_new.iterrows():\n",
    "    outStage = \"\"\n",
    "    if row['floofer'] != 'None':\n",
    "        outStage = 'Floofer'\n",
    "    if row['puppo'] != 'None':\n",
    "        outStage = 'Puppo'\n",
    "    if row['pupper'] != 'None':\n",
    "        outStage = 'Pupper'\n",
    "    if row['doggo'] != 'None':\n",
    "        outStage = 'Doggo'\n",
    "    if outStage != \"\":\n",
    "        archive_new.loc[index, 'dog_stage'] = outStage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_new.sample(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__:\n",
    "\n",
    "9- Drop the old columns that had the dog stages in them,.\n",
    "\n",
    "__Code__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',40) #Making things easier to see\n",
    "\n",
    "#That seemed to work! Time to get rid of the old columns\n",
    "\n",
    "archive_new.drop(['floofer', 'puppo', 'pupper','doggo'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_new.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up a bit\n",
    "archive_clean = archive_new.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there is ofcourse many more things to do, but for now we have dealt with:\n",
    "\n",
    "- Removing names that were not names\n",
    "- Removing links from text field\n",
    "- Remove retweets\n",
    "- Remove Replies\n",
    "- Remove columns for retweets and replies\n",
    "- Convert the four stages to a single column\n",
    "- Change names that were 'None' to actual None's\n",
    "- Remove now useless columns stating the stages\n",
    "- Copied to archive_clean\n",
    "\n",
    "\n",
    "# Img_pred wrangling\n",
    "\n",
    "image predictions\n",
    "\n",
    "my plan here is to deal with these issues:\n",
    "- remove predictions for non-dogs\n",
    "- have just one prediction with a confidence level per dog\n",
    "- remove img_num column\n",
    "- fix upper and lower cases in prediction names to all lower case, just for conformity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_clean = img_pred.copy()\n",
    "img_pred_clean.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__:\n",
    "\n",
    "10a- Clean up the predictions by keeping just the highest rated prediction that is also a dog. If none of the predictions are dogs, the predition will be set to None with a confidence of 0. \n",
    "10b- While running through the data we will convert to lowercase.\n",
    "\n",
    "__Code__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a row for the dog race and confidence\n",
    "img_pred_clean['dog_type'] = None\n",
    "img_pred_clean['type_confidence'] = 0.0\n",
    "#Now, we grab the highest confidence prediction, that is a dog. \n",
    "\n",
    "#This is 10a\n",
    "for index, row in img_pred_clean.iterrows():\n",
    "    outPred = \"\"\n",
    "    outConf = 0.0\n",
    "    if row['p1_dog'] == True:\n",
    "        outPred = row['p1']\n",
    "        outConf = row['p1_conf']\n",
    "    elif row['p2_dog'] == True:\n",
    "        outPred = row['p2']\n",
    "        outConf = row['p2_conf']\n",
    "    elif row['p3_dog'] == True:\n",
    "        outPred = row['p3']\n",
    "        outConf = row['p3_conf']\n",
    "    else:\n",
    "        outPred = None\n",
    "        outConf = 0.0\n",
    "    \n",
    "    #Before Storing convert to lower case if not None\n",
    "    #This is 10b\n",
    "    if outPred:\n",
    "        outPred = outPred.lower()\n",
    "    img_pred_clean.loc[index, 'dog_type'] = outPred\n",
    "    img_pred_clean.loc[index, 'type_confidence'] = outConf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_clean.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__:\n",
    "\n",
    "11- drop columns from the old predictions\n",
    "\n",
    "__Code__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns for predictions\n",
    "img_pred_clean.drop(['p1','p1_conf', 'p1_dog','p2','p2_conf', 'p2_dog','p3','p3_conf', 'p3_dog'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define__:\n",
    "\n",
    "12- Remove the img_num colum\n",
    "\n",
    "__Code__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and lets drop the img_num column as well\n",
    "img_pred_clean.drop(['img_num'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_clean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_clean.dog_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the merging begin! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_pred_clean.head()\n",
    "#archive_clean.info()\n",
    "\n",
    "archive_merge = archive_clean.merge(img_pred_clean , on='tweet_id')\n",
    "archive_merge.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And lets add the data from the twitter api\n",
    "\n",
    "archive_merge = archive_merge.merge(tw_api_data , on='tweet_id')\n",
    "archive_merge.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just in case I need it, I'll calculate the rating\n",
    "archive_merge['rating'] = archive_merge['rating_numerator']/archive_merge['rating_denominator']\n",
    "archive_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even though there is still plenty of things to do and doublecheck, I am satisfied with the dataset I have now. So it's time to move on to...\n",
    "\n",
    "# Analysis\n",
    "\n",
    "First of i'll store the new dataset so it's easier to work with from here, and i'll load that back into a new dataframe.\n",
    "\n",
    "Then I'll take a look at the data and see what insights I can get from it, and document and visualize these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_merge.to_csv('twitter_archive_master.csv', index = None, header=True)\n",
    "data_clean = pd.read_csv('twitter_archive_master.csv')\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.hist(figsize=(28,14), bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see what happens when we plot the rating againts favourited.\n",
    "\n",
    "plt.figure(figsize =(20,10))\n",
    "plt.scatter((data_clean['rating']), data_clean['favorite_count'].transform(lambda x: np.log10(x)),s=10)\n",
    "plt.title('rating vs favourites')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('favourited')\n",
    "plt.xlim(-1,3) #in dialing this in, I had to take the outliers out of the picture to see what was going on\n",
    "\n",
    "plt.savefig('rate_v_favor.png')\n",
    "plt.show()\n",
    "data_clean.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a clear correlation but it does seem that you get more favourites from ratings over 1.0 and that once the rating gets to 1.5 or higher, favouriting ceases. This can be obeserved in the graph from the clustering but does not appear in the correlation matrix.\n",
    "\n",
    "_An observation_\n",
    "\n",
    "It seems that nothing in the dataframe correlates linearly, which actually makes sense, as there are no rules for how ratings are given per se, so it is very much a case of controlled randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe looking at dog stages can lead to something interesting?\n",
    "\n",
    "stage_rate = data_clean.groupby(['dog_stage','rating'])\n",
    "stage_rate.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be something there... let's keep looking at the bigger picture first though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data_clean, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there seems to be a few things that jump out, I cannot use the rating column against the original ratings, since it's derrived from there. Also, the tweet_id and type_confidence can't really say anything of value here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maybe looking over time can help us learn something\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "\n",
    "#Time format: 2017-07-30 15:58:51 +0000\n",
    "\n",
    "# Define the date format\n",
    "myFmt = DateFormatter('%Y-%m-%d') \n",
    "\n",
    "data_clean['short_timestamp'] = data_clean['timestamp'].astype('datetime64[ns]')\n",
    "\n",
    "# plot the data\n",
    "fig, (ax, ax2) = plt.subplots(2, figsize=(15, 9))\n",
    "\n",
    "ax.plot(data_clean.short_timestamp, data_clean.favorite_count.rolling(90, win_type='triang').sum())\n",
    "ax2.plot(data_clean.short_timestamp, data_clean.rating.rolling(90, win_type='triang').sum())\n",
    "\n",
    "ax.set_title('Development of favourite counts over time')\n",
    "ax.set(xlabel=\"Time\", ylabel=\"Favourite Counts\",)\n",
    "ax.xaxis_date()\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "ax2.set_title('Ratingss over time')\n",
    "ax2.set(xlabel=\"Time\", ylabel=\"Rating\",)\n",
    "ax2.xaxis_date()\n",
    "ax2.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "fig.savefig('rate_v_favor_over_time.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that while favourites increase over time, so does the ratings of dogs. The two major spikes seen in the ratings can also be seen in the favourites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_clean[data_clean.dog_stage == 'Pupper']['rating'].hist()\n",
    "fig, ([ax, ax2],[ax3,ax4]) = plt.subplots(nrows=2, ncols=2, figsize=(18, 9))\n",
    "\n",
    "ax.hist(data_clean[data_clean.dog_stage == 'Pupper']['rating'])\n",
    "ax2.hist(data_clean[data_clean.dog_stage == 'Puppo']['rating'])\n",
    "ax3.hist(data_clean[data_clean.dog_stage == 'Doggo']['rating'])\n",
    "ax4.hist(data_clean[data_clean.dog_stage == 'Floofer']['rating'])\n",
    "ax.set_title('Pupper')\n",
    "ax2.set_title('Puppo')\n",
    "ax3.set_title('Doggo')\n",
    "ax4.set_title('Floofer')\n",
    "ax.set(xlabel=\"Rating\", ylabel=\"Counts\",)\n",
    "ax2.set(xlabel=\"Rating\", ylabel=\"Counts\",)\n",
    "ax3.set(xlabel=\"Rating\", ylabel=\"Counts\",)\n",
    "ax4.set(xlabel=\"Rating\", ylabel=\"Counts\",)\n",
    "\n",
    "fig.savefig('rate_v_dogStage.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, from this little graph we can see a few interesting things as well. First off,if we take the outliers in Puppers out of the picture, all the ratings seem to be left-skewed. That means that people tend to rate their dogs higher when measured agaist the average. We can also see that owners of Puppers are more likely to actually rate their dogs in an 'expected' manner, with a greater deal of ratings not going over 1.0, though it's not something they adhere to in general. \n",
    "The insights gained is that people tend to rate their dogs not just above average (at least in the sense that an average rating should fall withing 0-1.0) but slightly above 1, leaning towards higher ratings. Interestingly enough, the dog owners are somewhat concious enough to not overwhelmingly rate their dogs at much more than 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
